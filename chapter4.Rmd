###Week 4: Clustering
This week we are going to do clustering.

Load MASS, tidyr, dplyr, corrplot, reshape2 and ggplot2 packages
```{r}
library(MASS)
library(tidyr)
library(ggplot2)
library(dplyr)
library(corrplot)
library(reshape2)
```

Load the Boston data, print out structure, summary, matrix plot and correlation plot of the variables.

```{r}
# load the data
data("Boston")

# explore the dataset
str(Boston)
summary(Boston)

# plot matrix of the variables
d <- melt(Boston)
ggplot(d,aes(x = value)) + 
    facet_wrap(~variable,scales = "free") + 
    geom_histogram()
pairs(Boston)

# calculate the correlation matrix and round it
cor_matrix <- cor(Boston) %>% round(digits = 2)

# print the correlation matrix
corrplot(cor_matrix, type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)

# visualize the correlation matrix
corrplot(cor_matrix, method="circle")

```

The Boston data has 506 rows and 14 columns. The data is about Housing values in Boston and contains variables that may explain some of the variance in the housing values, such as distances to five Boston employment centres and teacher-pupil ratios of the town. Many variables have a bit strange distributions: huge spikes and otherwise fairly low numbers.

As we will be dealing with the crime rate variable, lets discuss it a bit. Crime rate clearly is very high in some areas, crime does not distribute evenly across the ciry

It is not suprising that business areas have higher amount of pollution. Crime rate is negatively correlated with at least distances to employment centres and positively to access to radial highways. Also, richer areas, where the valuable homes are, seem to have less crime.

Explanations of variables, as they are not named intuitively:

crim = per capita crime rate by town.
zn = proportion of residential land zoned for lots over 25,000 sq.ft.
indus = proportion of non-retail business acres per town.
chas = Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
nox = nitrogen oxides concentration (parts per 10 million).
rm = average number of rooms per dwelling.
age = proportion of owner-occupied units built prior to 1940.
dis = weighted mean of distances to five Boston employment centres.
rad = index of accessibility to radial highways.
tax = full-value property-tax rate per \$10,000.
ptratio = pupil-teacher ratio by town.
black = 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
lstat = lower status of the population (percent).
medv = median value of owner-occupied homes in \$1000s.

#Standardizing the data

As we are going to use LDA, we need to scale the data so that it is normally distributed and each variable has the same variance.

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
```

Create a categorical variable out of crim and drop the original. LDA is used to classify categorical variables, so we will turn the crime rate into such. At this point, I will create a copy of standardized data with original crim for the k-means clustering exercise and name it b_scaled2. This way there is no need to reload the data and do the scaling again later on.

```{r}
# save the scaled crim as scaled_crim and create copy of scaled data
scaled_crim  <- boston_scaled$crim
b_scaled2 <- boston_scaled
# summary of the scaled_crim
summary(scaled_crim)

# create a quantile vector of crim and print it
bins <- quantile(scaled_crim)
bins


# create a categorical variable 'crime'
crime <- cut(scaled_crim, breaks = bins, include.lowest = TRUE, label=c("low","med_low","med_high","high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

Create training and test sets out of the scaled data. We will later test our model using the test dataset: we will create the model with the training set. Crime variable is removed from the test dataset.

```{r}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)



```

Fit the linear discriminant analysis on the train set. Here I use the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables. After this, the results are plotted.

```{r}

# linear discriminant analysis
lda.fit <- lda(crime ~., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 1)
```
The plot shows that index of accessibility to radial highways has greatest impact on classification of high crime rate areas.

Now I will predict the classes with the LDA model on the test data and cross tabulate the results with the crime categories from the test set. The categories were saved earlier to correct_classes

```{r}


# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```

Model is fairly accurate with its prediction. Noteworthy is that it did not do any mistakes with high crime area predicitions. It is more confused with low and med_low categories.

#K-means clustering

Now I will do K-means clustering to the scaled b_scaled data that we saved earlier. I will calculate the distances between the observations and run k-means algorithm on the dataset. After this, I will investigate what is the optimal number of clusters and run the algorithm again. 

```{r}
# euclidean distance matrix
dist_eu <- dist(b_scaled2)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(b_scaled2, method="manhattan")

# look at the summary of the distances
summary(dist_man)

# k-means clustering
km <-kmeans(dist_eu, centers = 15)

# plot the scaled Boston dataset with clusters
pairs(b_scaled2, col = km$cluster)

#Optimal cluster amount
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})

# visualize the results
plot(1:k_max, twcss, type='b')


```
With 15 clusters, it is quite hard to make any sense out of the pairs plot.

After calculating total within sum of squares and plotting it, sharpest drop is between 1 and 2, so 2 is probably the optimal cluster amount.

```{r}
# k-means clustering
km <-kmeans(dist_eu, centers = 2)


# plot the scaled Boston dataset with clusters
pairs(b_scaled2, col = km$cluster)
```
Index of accessibility to radial highways seems divide these clusters apart. Regarding the crime rate, one cluster has only low crime rate in it, another one both high and low. Proportion of non-retail businesses also seems to be a clear divider. It would be interesting to see these clusters plotted to an actual map.
